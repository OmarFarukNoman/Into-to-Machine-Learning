{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkNElYI5/4hkTP3lITgsHW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarFarukNoman/Into-to-Machine-Learning/blob/main/Homework-5/Problem_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem-3(a)**"
      ],
      "metadata": {
        "id": "y58dMDA2ZcAM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zakkFtNLW-6d",
        "outputId": "d27902b6-eb77-4b4f-bc03-0338fd22087d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network Result after 200 iterations:\n",
            "\n",
            "Training Time: 0.73 seconds\n",
            "Final Training Loss (MSE): 26305951170560.00\n",
            "Final Validation Loss (MSE): 25845322219520.00\n",
            "Training RMSE: 5128933.00\n",
            "Validation RMSE: 5083829.50\n",
            "Training R² Score: -6.1883\n",
            "Validation R² Score: -8.1609\n",
            "\n",
            "Model Architecture:\n",
            "Input Features: 5 features\n",
            "Input Layer: 6 nodes\n",
            "Hidden Layer: 8 nodes with ReLU activation\n",
            "Output Layer: 1 node (linear)\n",
            "Total Parameters: 57\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "\n",
        "# We're using housing dataset for HW5\n",
        "url = \"https://raw.githubusercontent.com/HamedTabkhi/Intro-to-ML/main/Dataset/Housing.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# I've separated features and target\n",
        "features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
        "target = 'price'\n",
        "\n",
        "X_in = df[features].values\n",
        "y_out = df[target].values\n",
        "\n",
        "# Split data using PyTorch (80% training, 20% validation)\n",
        "torch.manual_seed(42)          # for reproducibility\n",
        "N = X_in.shape[0]\n",
        "train_size = int(0.8 * N)\n",
        "\n",
        "# Convert to tensors first\n",
        "X_in_tensor = torch.FloatTensor(X_in)\n",
        "y_out_tensor = torch.FloatTensor(y_out).reshape(-1, 1)\n",
        "\n",
        "# Create random permutation of indices\n",
        "indices = torch.randperm(N)\n",
        "train_idx = indices[:train_size]\n",
        "val_idx   = indices[train_size:]\n",
        "\n",
        "# Split the tensors directly\n",
        "X_in_train = X_in_tensor[train_idx]\n",
        "X_in_val   = X_in_tensor[val_idx]\n",
        "y_out_train = y_out_tensor[train_idx]\n",
        "y_out_val   = y_out_tensor[val_idx]\n",
        "\n",
        "# Standardization (convert back to numpy for sklearn, then back to tensor)\n",
        "scaler = StandardScaler()\n",
        "X_in_train_scaled = scaler.fit_transform(X_in_train.numpy())\n",
        "X_in_val_scaled = scaler.transform(X_in_val.numpy())\n",
        "\n",
        "# Convert back to PyTorch tensors\n",
        "X_in_train_tensor = torch.FloatTensor(X_in_train_scaled)\n",
        "y_out_train_tensor = y_out_train.clone()\n",
        "X_in_val_tensor = torch.FloatTensor(X_in_val_scaled)\n",
        "y_out_val_tensor = y_out_val.clone()\n",
        "\n",
        "# Neural Network with one hidden layer (8 nodes)\n",
        "class HousingNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(HousingNN, self).__init__()\n",
        "        self.hidden = nn.Linear(input_dim, 8)  # One hidden layer with 8 nodes\n",
        "        self.output = nn.Linear(8, 1)          # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.hidden(x))  # ReLU activation for hidden layer\n",
        "        x = self.output(x)              # Linear output for regression\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "input_dim = len(features)\n",
        "model = HousingNN(input_dim)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 200\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    train_outputs = model(X_in_train_tensor)\n",
        "    train_loss = criterion(train_outputs, y_out_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_in_val_tensor)\n",
        "        val_loss = criterion(val_outputs, y_out_val_tensor)\n",
        "\n",
        "    # Store losses\n",
        "    train_losses.append(train_loss.item())\n",
        "    val_losses.append(val_loss.item())\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Final evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Final predictions\n",
        "    final_train_outputs = model(X_in_train_tensor)\n",
        "    final_val_outputs = model(X_in_val_tensor)\n",
        "\n",
        "    final_train_loss = criterion(final_train_outputs, y_out_train_tensor).item()\n",
        "    final_val_loss = criterion(final_val_outputs, y_out_val_tensor).item()\n",
        "\n",
        "    # Calculate RMSE for accuracy metric\n",
        "    train_rmse = torch.sqrt(criterion(final_train_outputs, y_out_train_tensor)).item()\n",
        "    val_rmse = torch.sqrt(criterion(final_val_outputs, y_out_val_tensor)).item()\n",
        "\n",
        "    # Calculate R-squared for accuracy\n",
        "    train_mean = y_out_train_tensor.mean()\n",
        "    train_ss_total = ((y_out_train_tensor - train_mean) ** 2).sum()\n",
        "    train_ss_residual = ((y_out_train_tensor - final_train_outputs) ** 2).sum()\n",
        "    train_r2 = 1 - (train_ss_residual / train_ss_total).item()\n",
        "\n",
        "    val_mean = y_out_val_tensor.mean()\n",
        "    val_ss_total = ((y_out_val_tensor - val_mean) ** 2).sum()\n",
        "    val_ss_residual = ((y_out_val_tensor - final_val_outputs) ** 2).sum()\n",
        "    val_r2 = 1 - (val_ss_residual / val_ss_total).item()\n",
        "\n",
        "# Results\n",
        "print(\"Neural Network Result after 200 iterations:\")\n",
        "print(f\"\\nTraining Time: {training_time:.2f} seconds\")\n",
        "print(f\"Final Training Loss (MSE): {final_train_loss:.2f}\")\n",
        "print(f\"Final Validation Loss (MSE): {final_val_loss:.2f}\")\n",
        "print(f\"Training RMSE: {train_rmse:.2f}\")\n",
        "print(f\"Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"Training R² Score: {train_r2:.4f}\")\n",
        "print(f\"Validation R² Score: {val_r2:.4f}\")\n",
        "\n",
        "# Model architecture info\n",
        "print(f\"\\nModel Architecture:\")\n",
        "print(f\"Input Features: {input_dim} features\")\n",
        "print(f\"Input Layer: {input_dim+1} nodes\") # 1 is added for bias term\n",
        "print(f\"Hidden Layer: 8 nodes with ReLU activation\")\n",
        "print(f\"Output Layer: 1 node (linear)\")\n",
        "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem-3(b)**"
      ],
      "metadata": {
        "id": "ZMmpGR7mZhn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# We're using housing dataset for HW5\n",
        "url = \"https://raw.githubusercontent.com/HamedTabkhi/Intro-to-ML/main/Dataset/Housing.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# I've separated features and target\n",
        "features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
        "target = 'price'\n",
        "\n",
        "X_in = df[features].values\n",
        "y_out = df[target].values\n",
        "\n",
        "# Split data using PyTorch (80% training, 20% validation)\n",
        "torch.manual_seed(42)          # for reproducibility\n",
        "N = X_in.shape[0]\n",
        "train_size = int(0.8 * N)\n",
        "\n",
        "# Convert to tensors first\n",
        "X_in_tensor = torch.FloatTensor(X_in)\n",
        "y_out_tensor = torch.FloatTensor(y_out).reshape(-1, 1)\n",
        "\n",
        "# Create random permutation of indices\n",
        "indices = torch.randperm(N)\n",
        "train_idx = indices[:train_size]\n",
        "val_idx   = indices[train_size:]\n",
        "\n",
        "# Split the tensors directly\n",
        "X_in_train = X_in_tensor[train_idx]\n",
        "X_in_val   = X_in_tensor[val_idx]\n",
        "y_out_train = y_out_tensor[train_idx]\n",
        "y_out_val   = y_out_tensor[val_idx]\n",
        "\n",
        "# Standardization (convert back to numpy for sklearn, then back to tensor)\n",
        "scaler = StandardScaler()\n",
        "X_in_train_scaled = scaler.fit_transform(X_in_train.numpy())\n",
        "X_in_val_scaled = scaler.transform(X_in_val.numpy())\n",
        "\n",
        "# Convert back to PyTorch tensors\n",
        "X_in_train_tensor = torch.FloatTensor(X_in_train_scaled)\n",
        "y_out_train_tensor = y_out_train.clone()\n",
        "X_in_val_tensor = torch.FloatTensor(X_in_val_scaled)\n",
        "y_out_val_tensor = y_out_val.clone()\n",
        "\n",
        "# Neural Network with 3 hidden layers\n",
        "class HousingNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(HousingNN, self).__init__()\n",
        "        self.hidden1 = nn.Linear(input_dim, 16)  # First hidden layer with 16 nodes\n",
        "        self.hidden2 = nn.Linear(16, 8)          # Second hidden layer with 8 nodes\n",
        "        self.hidden3 = nn.Linear(8, 4)           # Third hidden layer with 4 nodes\n",
        "        self.output = nn.Linear(4, 1)            # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.hidden1(x))  # ReLU activation for first hidden layer\n",
        "        x = torch.relu(self.hidden2(x))  # ReLU activation for second hidden layer\n",
        "        x = torch.relu(self.hidden3(x))  # ReLU activation for third hidden layer\n",
        "        x = self.output(x)               # Linear output for regression\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "input_dim = len(features)\n",
        "model = HousingNN(input_dim)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 200\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    train_outputs = model(X_in_train_tensor)\n",
        "    train_loss = criterion(train_outputs, y_out_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_in_val_tensor)\n",
        "        val_loss = criterion(val_outputs, y_out_val_tensor)\n",
        "\n",
        "    # Store losses\n",
        "    train_losses.append(train_loss.item())\n",
        "    val_losses.append(val_loss.item())\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Final evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Final predictions\n",
        "    final_train_outputs = model(X_in_train_tensor)\n",
        "    final_val_outputs = model(X_in_val_tensor)\n",
        "\n",
        "    final_train_loss = criterion(final_train_outputs, y_out_train_tensor).item()\n",
        "    final_val_loss = criterion(final_val_outputs, y_out_val_tensor).item()\n",
        "\n",
        "    # Calculate RMSE for accuracy metric\n",
        "    train_rmse = torch.sqrt(criterion(final_train_outputs, y_out_train_tensor)).item()\n",
        "    val_rmse = torch.sqrt(criterion(final_val_outputs, y_out_val_tensor)).item()\n",
        "\n",
        "    # Calculate R-squared for accuracy\n",
        "    train_mean = y_out_train_tensor.mean()\n",
        "    train_ss_total = ((y_out_train_tensor - train_mean) ** 2).sum()\n",
        "    train_ss_residual = ((y_out_train_tensor - final_train_outputs) ** 2).sum()\n",
        "    train_r2 = 1 - (train_ss_residual / train_ss_total).item()\n",
        "\n",
        "    val_mean = y_out_val_tensor.mean()\n",
        "    val_ss_total = ((y_out_val_tensor - val_mean) ** 2).sum()\n",
        "    val_ss_residual = ((y_out_val_tensor - final_val_outputs) ** 2).sum()\n",
        "    val_r2 = 1 - (val_ss_residual / val_ss_total).item()\n",
        "\n",
        "# Overfitting Analysis\n",
        "overfitting_gap = final_train_loss - final_val_loss\n",
        "overfitting_percentage = (abs(overfitting_gap) / min(final_train_loss, final_val_loss)) * 100\n",
        "\n",
        "# Performance gap analysis\n",
        "train_val_gap = abs(train_rmse - val_rmse)\n",
        "r2_gap = abs(train_r2 - val_r2)\n",
        "\n",
        "# Results\n",
        "print(\"Neural Network Result after 200 iterations:\")\n",
        "print(f\"\\nTraining Time: {training_time:.2f} seconds\")\n",
        "print(f\"Final Training Loss (MSE): {final_train_loss:.2f}\")\n",
        "print(f\"Final Validation Loss (MSE): {final_val_loss:.2f}\")\n",
        "print(f\"Training RMSE: {train_rmse:.2f}\")\n",
        "print(f\"Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"Training R² Score: {train_r2:.4f}\")\n",
        "print(f\"Validation R² Score: {val_r2:.4f}\")\n",
        "\n",
        "# Overfitting Analysis Results\n",
        "print(f\"\\nOverfitting Analysis:\")\n",
        "print(f\"Training-Validation Loss Gap: {overfitting_gap:.4f}\")\n",
        "print(f\"Performance Gap Percentage: {overfitting_percentage:.2f}%\")\n",
        "print(f\"RMSE Gap: {train_val_gap:.4f}\")\n",
        "print(f\"R² Score Gap: {r2_gap:.4f}\")\n",
        "\n",
        "# Model architecture info\n",
        "print(f\"\\nModel Architecture:\")\n",
        "print(f\"Input Features: {input_dim} features\")\n",
        "print(f\"Input Layer: {input_dim+1} nodes\")\n",
        "print(f\"Hidden Layer 1: 16 nodes with ReLU activation\")\n",
        "print(f\"Hidden Layer 2: 8 nodes with ReLU activation\")\n",
        "print(f\"Hidden Layer 3: 4 nodes with ReLU activation\")\n",
        "print(f\"Output Layer: 1 node (linear)\")\n",
        "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")\n"
      ],
      "metadata": {
        "id": "cYYzriWlhIeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16249296-e5ee-45cb-8a0c-11c634b971da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network Result after 200 iterations:\n",
            "\n",
            "Training Time: 0.99 seconds\n",
            "Final Training Loss (MSE): 26305831632896.00\n",
            "Final Validation Loss (MSE): 25845208973312.00\n",
            "Training RMSE: 5128921.00\n",
            "Validation RMSE: 5083818.50\n",
            "Training R² Score: -6.1883\n",
            "Validation R² Score: -8.1608\n",
            "\n",
            "Overfitting Analysis:\n",
            "Training-Validation Loss Gap: 460622659584.0000\n",
            "Performance Gap Percentage: 1.78%\n",
            "RMSE Gap: 45102.5000\n",
            "R² Score Gap: 1.9725\n",
            "\n",
            "Model Architecture:\n",
            "Input Features: 5 features\n",
            "Input Layer: 6 nodes\n",
            "Hidden Layer 1: 16 nodes with ReLU activation\n",
            "Hidden Layer 2: 8 nodes with ReLU activation\n",
            "Hidden Layer 3: 4 nodes with ReLU activation\n",
            "Output Layer: 1 node (linear)\n",
            "Total Parameters: 273\n"
          ]
        }
      ]
    }
  ]
}